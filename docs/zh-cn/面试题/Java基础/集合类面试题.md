# 集合框架

以下问题，默认以JDK1.8为标准。



## 概要

### 对Java集合类有哪些了解呢？
JAVA集合框架主要由两个接口派生而出，分别是Collection和Map接口，这是集合框架的两个根接口。其中Collection接口的实现类直接保存对象，而Map接口的实现类是用键值对(key-value)的形式保存数据的，可以根据key值获取相应的value值。 其中Collection又派生出List、Set、Queue等三大体系。
![Collection接口关系图](https://gitee.com/guohangbk/resources/raw/master/images/blog/202110162117026.png)
List主要用来存放一些有序、可重复的元素；主要实现类有：ArrayList、LinkedList、Vector和Stack。

1. ArrayList是基于数组实现的，增删改比较慢，查询比较快。 
2. LinkedList是基于链表实现的，与ArrayList正好相反，它的增删改比较快，查询比较慢。
3. Vector也是基于数组实现的，但它是线程安全的类，一般不推荐使用。
4. Stack是Vector的派生类，是“栈”数据结构的应用，具有后进显出的特点，也是线程安全的，一般不推荐使用，因为效率比较低。（那么在多线程环境中，如何保证线程安全那？通过代码来保证同步）

Set主要用来存放一些无序、不可重复的元素。主要实现类有：HashSet、TreeSet、LinkedHashSet

1. HashSet按照Hash算法来存储集合元素，具有很好的存取和查找性能，常用。

2. TreeSet是SortedSet接口的实现类，采用红黑树数据结构存储集合元素，可以保证集合元素处于有序状态，不是元素插入顺序，而是元素实际大小数据。

3. LinkedHashSet是HashSet的子类，根据Hash值决定元素存放位置，同时使用链表维护元素次序，以插入顺序保存元素。性能低于HashSet，但是迭代访问Set里全部元素时具有很好的性能。

Queue是基于队列实现的，通常采用先进先出存储元素。主要实现类有PriorityQueue、ArrayDeque实现类。

1. PriorityQueue 标准的队列实现类，按照元素实际大小重新排序保存。最后取出的是最小元素，而不是最先插入的元素。

2. ArrayDeque 基于数组实现的双端队列。



![img](https://gitee.com/guohangbk/resources/raw/master/images/blog/202110162129027.png)

Map 保存的键值对数组。主要实现类有HashMap、HashTable、LinkedHashMap.

1. HashMap 非线程安全类，可以使用Null作为键或值。
2. HashTable 线程安全类，不能使用Null作为键或值。
3. LinkedHashMap.是HashMap的子类。





## List接口

### 对List的了解

List在Java中是常用的集合接口，常见的实现类有ArrayList和LinkedList，ArrayList是在我开发中最常用的。

ArrayList是基于数组实现的，增删改比较慢，查询比较快。LinkedList是基于链表实现的，与ArrayList正好相反，它的增删改比较快，查询比较慢。



### Java本身就有数组了，为什么要用ArrayList呢？

原生的数组会有一个特点：你在使用的时候必须要为它创建大小，而ArrayList不用。在日常开发的时候，往往我们是不知道数组的大小的，如果数组的大小指定多了，内存浪费；如果数组大小指定少了，装不下。

假设我们给定数组的大小是10，要往这个数组里边填充元素，我们只能添加10个元素。而ArrayList不一样，ArrayList我们在使用的时候可以往里边添加20个，30个，甚至更多的元素，因为ArrayList是实现了动态扩容的。



### ArrayList动态扩容机制

当我们new ArrayList()的时候，默认会有一个空的Object数组，大小为0。在添加第一个元素的时候，会将这个空数组初始化为10。在新增的时候，会先计算容量够不够，如果容量够用，就会往list中添加数据。如果容量不够用了，就会执行扩容操作。

扩容操作是每次扩容为原来容量的1.5倍，公式为扩容后的大小 = 原始大小 + 原始大小 * 0.5。

因为扩容也是有性能开支的，所以如果在一开始就知道这个list会达到多少容量，可以在初始化的时候就指定List的大小。



### Vector你了解吗？

Vector也是基于数组实现的，它是线程安全的类，为什么说是线程安全呢？因为在Vector类中的大部分主要方法都是synchronized修饰的，所以这导致了它的效率是低的。它的扩容机制是每次扩容为原来容量的两倍。由于Vector的效率比较低，所以目前我们开发中基本是不用的。



### 如果需要线程安全的List，应该怎么办？

除了Vector之外，我们也可以用Collections来将ArrayList来包装一下，变成线程安全。

另外在java.util.concurrent包下还有一个类，叫做CopyOnWriteArrayList，是一个线程安全的List，底层是通过复制数组的方式来实现的。

为什么说是线程安全的呢？是因为在这个类的add方法中会有lock操作，然后会复制出一个新的数组，往新的数组里边add真正的元素，最后把array的指向改变为新的数组。get()方法又或是size()方法只是获取array所指向的数组的元素或者大小，是不加锁的。这个类没有扩容操作，添加一个元素就做一次创建数组（容量是原来的+1）并拷贝。

![image-20211016221520846](https://gitee.com/guohangbk/resources/raw/master/images/blog/202110162215139.png)



### CopyOnWriteArrayList有什么缺点吗？

CopyOnWriteArrayList是很耗费内存的，每次set()/add()都会复制一个数组出来。另外就是CopyOnWriteArrayList只能保证数据的最终一致性，不能保证数据的实时一致性。假设两个线程，线程A去读取CopyOnWriteArrayList的数据，还没读完，现在线程B把这个List给清空了，线程A此时还是可以把剩余的数据给读出来。





## Map接口

### 对Map的了解

Map在Java里边是一个接口，常见的实现类有HashMap、LinkedHashMap、TreeMap和ConcurrentHashMap

HashTable的底层结构是数组+链表，线程安全的。

HashMap的底层结构是数组+链表/红黑树，线程不安全。

LinkedHashMap底层数据结构是数组+链表/红黑树+双向链表

TreeMap底层数据结构是红黑树

ConcurrentHashMap底层数据结构也是数组+链表/红黑树



### HashMap的实现

HashMap是基于hashing的原理，用put(key, value)存储对象到HashMap中，用get(key)从HashMap中获取对象。

当我们给put()方法传递键和值时，我们先对键调用hashCode()方法，返回的hashCode用于找到bucket位置来储存Entry对象。这里关键点在于HashMap是在bucket中储存键对象和值对象，作为Map.Entry。

HashMap采用Entry数组来存储Key-Value对，每一个键值对组成了一个Entry实体，Entry类实际上是一个单向的链表结构，它具有Next指针，可以连接下一个Entry实体。HashMap采用了链地址法。链地址法，简单来说，就是数组加链表的结合，在每个数组元素上都有一个链表结构，当数据被Hash后，得到数组下标，把数据放在对应下标元素的链表上。

![image-20211016230735321](https://gitee.com/guohangbk/resources/raw/master/images/blog/202110171001441.png)

![image-20211016230841572](https://gitee.com/guohangbk/resources/raw/master/images/blog/202110171001683.png)





### New一个HashMap的时候，会发生什么

HashMap有几个构造方法，但最主要的就是指定初始值大小和负载因子的大小。如果我们不指定，默认HashMap的大小为16，负载因子的大小为0.75。

HashMap的大小只能是2次幂的，假设你传一个10进去，实际上最终HashMap的大小是16，你传一个7进去，HashMap最终的大小是8，具体的实现在tableSizeFor可以看到。

我们把元素放进HashMap的时候，需要算出这个元素所在的位置（hash）。在HashMap里用的是位运算来代替取模，能够更加高效地算出该元素所在的位置。



### HashMap在JDK1.7和1.8的区别？

![HashMap在JDK1.7和1.8的区别](https://gitee.com/guohangbk/resources/raw/master/images/blog/202110171142704.png)

1.8主要是进行了以下几个优化：

1. 数组+链表改成了数组+链表 / 红黑树；
2. 链表的插入方式从头插法改成了尾插法，简单说就是插入时，如果数组位置上已经有元素，1.7将新元素放到数组中，原始节点作为新节点的后继节点，1.8遍历链表，将元素放置到链表的最后；
3. 扩容的时候1.7需要对原数组中的元素进行重新hash定位在新数组的位置，1.8采用更简单的判断逻辑，位置不变或索引+旧容量大小；
4. 在插入时，1.7先判断是否需要扩容，再插入，1.8先进行插入，插入完成再判断是否需要扩容；



### 为什么HashMap中的链表需要转成红黑树？

因为Map中桶的元素初始化是链表保存的，当个数不多的时候，直接使用链表会更方便，实现起来会比较简单。链表的查找性能是O(n)，而树结构可以将查找性能提升到O(log(n))。

当链表长度很小的时候，即使遍历，速度也非常快，但是当链表长度不断变长，肯定会对查询性能有一定的影响，所以才需要转成树。

至于为什么阈值是8，是根据泊松分布来确定的。一个桶中链表长度大于8个元素的概率为0.00000006，是几乎不可能出现的事情。所以选择8是根据概率统计而选择的。

总结：防止发生hash冲突，链表长度过长，将时间复杂度由O(n)降为O(logn);



### 为什么HashMap中的链表转化为红黑树的值是8？

通过源码我们得知HashMap源码作者通过**泊松分布**算出，当桶中结点个数为8时，出现的几率是亿分之6的，因此常见的情况是桶中个数小于8的情况，此时链表的查询性能和红黑树相差不多，因为转化为树还需要时间和空间，所以此时没有转化成树的必要。

既然个数为8时发生的几率这么低，我们为什么还要当链表个数大于8时来树化来优化这几乎不会发生的场景呢？

首先我们要知道亿分之6这个几乎不可能的概率是建立在什么情况下的 答案是：建立在良好的hash算法情况下，例如String，Integer等包装类的hash算法、如果一旦发生桶中元素大于8，说明是不正常情况，可能采用了冲突较大的hash算法，此时桶中个数出现超过8的概率是非常大的，可能有n个key冲突在同一个桶中，此时再看链表的平均查询复杂度和红黑树的时间复杂度，就知道为什么要引入红黑树了，

举个例子，若hash算法写的不好，一个桶中冲突1024个key，使用链表平均需要查询512次，但是红黑树仅仅10次，红黑树的引入保证了在大量hash冲突的情况下，HashMap还具有良好的查询性能
![源码中注释为什么阈值 8](https://gitee.com/guohangbk/resources/raw/master/images/blog/202110171150900.png)



### HashMap扩容机制

HashMap初始化大小是16，负载因子默认是0.75（可以指定初始化大小和负载因子）。

在每次put新元素的时候，都会检测HashMap的大小有没有超过阈值（默认是当前大小*0.75），如果没超过则正常新增元素，如果超过这个阈值，则进行扩容，扩容的时候时候默认是扩**原来的2倍**。

负载因子调高了，这意味着哈希冲突的概率会增高，哈希冲突概率增高，同样会耗时（因为查找的速度变慢了）。

负载因子调低了，也会使得HashMap扩容操作变得更频繁，也会导致效率变低。



### 如果两个键的hashcode相同，如何获取值对象？ 

当我们调用get()方法，HashMap会使用键对象的hashcode找到bucket位置，然后获取值对象。如果有两个值对象储存在同一个bucket，将会遍历LinkedList直到找到值对象。

找到bucket位置之后，会调用keys.equals()方法去找到LinkedList中正确的节点，最终找到要找的值对象。(当程序通过 key 取出对应 value 时，系统只要先计算出该 key 的 hashCode() 返回值，在根据该 hashCode 返回值找出该 key 在 table 数组中的索引，然后取出该索引处的 Entry，最后返回该 key 对应的 value 即可。)



### HashMap扩容时，每个Enrty需要再计算一次吗？

在jdk7中,在resize的时候首先阈值是用newCapacity * loadFactor 。然后一个个的遍历Entry数组，然后看看里面的元素是否已经是一条链表了，如果是链表的话，那么就重新计算在新的table中的槽值。

java8在实现HashMap时做了一系列的优化，其中一个重要的优化即在扩容的时候，原有数组里的数据迁移到新数组里不需要重新hash，而是采用一种巧妙的方法。通过将数据的hash与扩容前的长度进行与操作，根据结果为0还是为1来做对应的处理e.hash & oldCap。如果是0，说明位置没有发生变化，如果是1，说明位置发生了变化，而且新的位置=老的位置+老的数组长度。



### HashMap是如何解决Hash冲突的？

在HashMap里用的是位运算来代替取模，能够更加高效地算出该元素所在的位置。只有大小为2次幂时，才能合理用位运算替代取模。

hash值冲突是发生在put()时，从源码可以看出，hash值是通过hash(key.hashCode())来获取的，当put的元素越来越多时，难免或出现不同的key产生相同的hash值问题，也即是hash冲突。

当拿到一个hash值，通过indexFor(hash, table.length)获取数组下标，先查询是否存在该hash值，若不存在，则直接以Entry<V,V>的方式存放在数组中。若存在，则再对比key是否相同；若hash值和key都相同，则替换value；若hash值相同，key不相同，则形成一个单链表，将hash值相同，key不同的元素以Entry<V,V>的方式存放在链表尾部，这样就解决了hash冲突，这种方法叫做**分离链表法**。

与之类似的方法还有一种叫做 **开放定址法**，开放定址法是采用线性探测（从相同hash值开始，继续寻找下一个可用的槽位）hashMap是数组，长度虽然可以扩大，但用线性探测法去查询槽位查不到时怎么办？因此hashMap采用了分离链表法。



### HashMap是不是线程安全的？

不是，在多线程环境下，1.7 会产生死循环、数据丢失、数据覆盖的问题，1.8 中会有数据覆盖和扩容出错的问题。

1.7的死循环是指在接近临界点时，若此时两个或者多个线程进行put操作，都会进行resize（扩容）和reHash（为key重新计算所在位置），而reHash在并发的情况下可能会形成链表环。总结来说就是在多线程环境下，使用HashMap进行put操作会引起死循环，导致CPU利用率接近100%。

1.8的数据覆盖，假如当A线程执行到下面代码第6行判断index位置为空后正好挂起，B线程开始执行第7 行，往index位置的写入节点数据，这时A线程恢复现场，执行赋值操作，就把A线程的数据给覆盖了；

1.8的扩容问题，++size这个地方也会造成多线程同时扩容等问题。

``` 
    final V putVal(int hash, K key, V value, boolean onlyIfAbsent,
                   boolean evict) {
        Node<K,V>[] tab; Node<K,V> p; int n, i;
        if ((tab = table) == null || (n = tab.length) == 0)
            n = (tab = resize()).length;
        if ((p = tab[i = (n - 1) & hash]) == null)
       		// 多线程执行到这里
            tab[i] = newNode(hash, key, value, null);
        else {
            Node<K,V> e; K k;
            if (p.hash == hash &&
                ((k = p.key) == key || (key != null && key.equals(k))))
                e = p;
            else if (p instanceof TreeNode)
                e = ((TreeNode<K,V>)p).putTreeVal(this, tab, hash, key, value);
            else {
                for (int binCount = 0; ; ++binCount) {
                    if ((e = p.next) == null) {
                        p.next = newNode(hash, key, value, null);
                        if (binCount >= TREEIFY_THRESHOLD - 1) // -1 for 1st
                            treeifyBin(tab, hash);
                        break;
                    }
                    if (e.hash == hash &&
                        ((k = e.key) == key || (key != null && key.equals(k))))
                        break;
                    p = e;
                }
            }
            if (e != null) { // existing mapping for key
                V oldValue = e.value;
                if (!onlyIfAbsent || oldValue == null)
                    e.value = value;
                afterNodeAccess(e);
                return oldValue;
            }
        }
        ++modCount;
        if (++size > threshold)
        	// 多个线程走到这，可能重复resize()
            resize();
        afterNodeInsertion(evict);
        return null;
    }
```



### 为什么1.7在并发执行put操作会引起死循环？

是因为多线程会导致HashMap的Entry链表形成环形数据结构，一旦形成环形数据结构，Entry的next节点永远不为空，就会产生死循环获取Entry。jdk1.7的情况下，并发扩容时容易形成链表环，此情况在1.8时就好太多太多了。因为在1.8中当链表长度大于阈值（默认长度为8）时，链表会被改成树形（红黑树）结构。


### 为什么包装类适合作为HashMap的Key？ 

因为他们一般不是不可变的，源码上面final，使用不可变类，而且重写了equals和hashcode方法，避免了键值对改写，提高HashMap性能。
String, Interger这样的wrapper类作为HashMap的键是再适合不过了，而且String最为常用。因为String是不可变的，也是final的，而且已经重写了equals()和hashCode()方法了。其他的wrapper类也有这个特点，不可变性是必要的。因为为了要计算hashCode()，就要防止键值改变，如果键值在放入时和获取时返回不同的hashcode的话，那么就不能从HashMap中找到你想要的对象。不可变性还有其他的优点，如线程安全。



### 在多线程环境下需要使用HashMap怎么办？

Java中有HashTable、Collections.synchronizedMap、以及ConcurrentHashMap可以实现线程安全的Map。

- HashTable是直接在操作方法上加synchronized关键字，锁住整个数组，粒度比较大，速度比较慢；
- Collections.synchronizedMap是使用Collections集合工具的内部类，通过传入Map封装出一个SynchronizedMap对象，内部定义了一个对象锁，方法内通过对象锁实现；
- ConcurrentHashMap使用分段锁，降低了锁粒度，让并发度大大提高。



### ConcurrentHashMap的分段锁的实现原理

#### JDK1.7

底层结构是分段的数组+链表，采用了**数组+Segment+分段锁**的方式实现线程安全。

ConcurrentHashMap使用分段锁技术，把整个Map分为N个Segment，将数据分成一段一段的存储，然后给每一段数据配一把锁，当一个线程占用锁访问其中一个段数据的时候，其他段的数据也能被其他线程访问，能够实现真正的并发访问。线程安全，但是效率提升N倍，默认提升16倍（读操作不加锁，由于HashEntry的value变量是 volatile的，也能保证读取到最新的值）。

定位一个元素的过程需要进行两次Hash操作+1次遍历操作。第一次Hash获取到segment，第二次Hash获取到是在table数组中某个具体的bucket，然后就在这个bucket的entry往后遍历对比equals值判断是否一致。

![JDK1.7的ConcurrentHashMap](https://gitee.com/guohangbk/resources/raw/master/images/blog/202110171537507.png)



#### JDK1.8

JDK8中彻底放弃了Segment转而采用的是Node，其设计思想也不再是JDK1.7中的分段锁思想。

底层数据结构是数组+链表/红黑树，内部大量采用**synchronized关键字、CAS操作和volatile关键字**保证线程安全和可见性。

ConcurrentHashMap能支持高并发的访问和更新，是线程安全的。通过在部分加锁和利用CAS算法来实现同步，在get的时候没有加锁，Node都用了volatile给修饰。在扩容时，会给每个线程分配对应的区间，并且为了防止putVal导致数据不一致，会给线程的所负责的区间加锁。

**Node：保存key，value及key的hash值的数据结构。其中value和next都用volatile修饰，保证并发的可见性。**

```
class Node<K,V> implements Map.Entry<K,V> {
    final int hash;
    final K key;
    volatile V val;
    volatile Node<K,V> next;
    //... 省略部分代码
} 
```

![JDK1.8的ConcurrentHashMap](https://gitee.com/guohangbk/resources/raw/master/images/blog/202110171619747.jpeg)

用到的并发技巧：

**volatile变量（sizeCtl）**：它是一个标记位，用来告诉其他线程这个坑位有没有人在，其线程间的可见性由volatile保证。

**CAS操作**：CAS操作保证了设置sizeCtl标记位的原子性，保证了只有一个线程能设置成功。



### ConcurrentHashMap在1.7和1.8的区别

JDK1.8版本的ConcurrentHashMap的数据结构已经接近HashMap，相对而言，ConcurrentHashMap只是增加了同步的操作来控制并发。

JDK1.7版本的ReentrantLock+Segment+HashEntry改变为JDK1.8版本中synchronized+CAS+Node+红黑树。

1. 数据结构：取消了Segment分段锁的数据结构，取而代之的是数组+链表+红黑树的结构。
2. 保证线程安全机制：JDK1.7采用segment的分段锁机制实现线程安全，其中segment继承自ReentrantLock。JDK1.8采用CAS+Synchronized保证线程安全。
3. 锁的粒度：原来是对需要进行数据操作的Segment加锁，现调整为对每个数组元素加锁（Node）。
4. 链表转化为红黑树：定位结点的hash算法简化会带来弊端，Hash冲突加剧，因此在链表节点数量大于8时，会将链表转化为红黑树进行存储。
5. 查询时间复杂度：从原来的遍历链表O(n)，变成遍历红黑树O(logN)。



### 如何用LinkedHashMap实现LRU？

LinkedHashMap通过维护双向链表保证了迭代顺序，该迭代顺序可以是插入顺序也可以是访问顺序，是通过accessOrder字段设置，true是基于访问排序，false为基于插入排序。所以我们可以将accessOrder设置为true，当某个位置的数据被命中，通过调整该数据的位置，将其移动至尾部（实际上是先删后插），新插入的元素也是直接放入尾部(尾插法)。这样一来，最近被命中的元素就向尾部移动，那么链表的头部就是最近最少使用的元素所在的位置。

如果不做任何操作的话，LinkedHashMap和HashMap一样容量不够就扩容。所以要重写 removeEldestEntry 方法（默认是return false），这样写的话，容量不够就会淘汰 oldest 的页面，从而跟LRU的思路是一致的。

![删除策略](https://gitee.com/guohangbk/resources/raw/master/images/blog/202110171652225.png)



### 讲一下CAS算法

CAS是compare and swap的缩写，即我们所说的比较交换。cas是一种基于锁的操作，而且是乐观锁。在java中锁分为乐观锁和悲观锁。悲观锁是将资源锁住，等之前获得锁的线程释放锁之后，下一个线程才可以访问。而乐观锁采取了一种宽泛的态度，通过某种方式不加锁来处理资源，比如通过给记录加version来获取数据，性能较悲观锁有很大的提高。

CAS 操作包含三个操作数 —— 内存位置（V）、预期原值（A）和新值(B)。如果内存地址里面的值和A的值是一样的，那么就将内存里面的值更新成B。CAS是通过无限循环来获取数据的，若果在第一轮循环中，a线程获取地址里面的值被b线程修改了，那么a线程需要自旋，到下次循环才有可能机会执行。







## Set接口

### 对Set的了解

Set接口完整的继承了Collection接口，也就是说Set集合几乎与Collection的操作是对等的。

从实际的开发来看，大量使用到Set集合的框架只有Hibernate；还有一些批量删除的功能也是通过Set集合实现的，因为大部分情况下优先考虑的还是List接口。

Set常用的三个子类是HashSet、TreeSet、LinkedHashSet。

#### HashSet：无序存放
（1）打开源代码之后发现整个HashSet类里面存在有一个HashMap，但是很明显，没使用这个Map的Value，只使用了Key（HashMap的主要特点是key不能重复）；
（2）在HashSet类执行add（）方法的时候发现是利用了Map接口中的put（）实现的；
（3）在整个存储过程之中存在有一个hashCode（）的使用；
（4）在内部的Node类处理时依然使用了equals（）判断key与value内容。

#### TreeSet：有序
（1） 里面存放的是TreeMap，TreeMap的特点在于所有的key都是可以排序的，排序的依据是Comparable（往往会忽略掉Comparator）；
（2） 在TreeMap之中对于大小的关系判断强制使用了Comparable接口中的compareTo（）完成。

#### LinkedHashSet：带有插入顺序的HashSet

根据元素的hashCode值来决定元素的存储位置，同时使用链表维护元素的次序，使得元素是以插入的顺序来保存的。当遍历LinkedHashSet集合里的元素时，LinkedHashSet将会按元素的添加顺序来访问集合里的元素。但是由于要维护元素的插入顺序，在性能上略低与HashSet，但在迭代访问Set里的全部元素时有很好的性能。
**注意：**LinkedHashSet依然不允许元素重复，判断重复标准与HashSet一致。





## HashSet实现原理？

1. 基于HashMap实现的，默认构造函数是构建一个初始容量为16，负载因子为0.75 的HashMap。封装了一个 HashMap 对象来存储所有的集合元素，所有放入 HashSet 中的集合元素实际上由 HashMap 的 key 来保存，而 HashMap 的 value 则存储了一个 PRESENT，它是一个静态的 Object 对象。
2. 当我们试图把某个类的对象当成 HashMap的 key，或试图将这个类的对象放入 HashSet 中保存时，需要判断key是否存在，重写元素的类的equals()和hashCode()方法。当向Set中添加对象时，首先调用此对象所在类的hashCode()方法，计算次对象的哈希值，此哈希值决定了此对象在Set中存放的位置；若此位置没有被存储对象则直接存储，若已有对象则通过对象所在类的equals()比较两个对象是否相同，相同则不能被添加。
3. 通常来说，所有参与计算 hashCode() 返回值的关键属性，都应该用于作为 equals() 比较的标准。HashSet的其他操作都是基于HashMap的。
   



